
* Graphs for DAGS




Four elemental confounds:

+ the fork
+ the pipe
+ the collider
+ the descendant.

You want to know the causal effect of X on Y

Sometimes multiple regression helps to solve these confounds

Sometime multiple refression gives exactly the wrong conclusion


** Fork

Z causes both X and Y, but there is no effect of X on Y once you condition on Z.

#+BEGIN_SRC ditaa :file Fork.png
+--------------+     +--------------+   
|              |     |              |
|      Z       |---->|       Y      | 
|              |     |              |
+--------------+     +--------------+
       |
       |
       |
       V
+--------------+
|              |
|      X       |
|              |
+--------------+


#+END_SRC

#+RESULTS:
[[file:Fork.png]]


** Pipe

#+BEGIN_SRC ditaa :file Pipe.png
+--------------+     +--------------+     +--------------+   
|              |     |              |     |              |
|      X       |---->|       Z      |---->|       Y      | 
|              |     |              |     |              |
+--------------+     +--------------+     +--------------+


#+END_SRC

#+RESULTS:
[[file:Pipe.png]]


** Collider

#+BEGIN_SRC ditaa :file Collider.png

+--------------+     +--------------+     +--------------+   
|              |     |              |     |              |
|      X       |---->|       Z      |<----|       Y      | 
|              |     |              |     |              |
+--------------+     +--------------+     +--------------+
                                  ^          ^
                                  |          |
                                  |          |
                                +--------------+
                                |              |
                                |      U       |
                                |              |
                                +--------------+
                                
#+END_SRC

#+RESULTS:
[[file:Collider.png]]

X = grandparent education
Y = children education
Z = parent education

Effect of U is called a "backdoor path".

There is a fourth unobserved effect: (educational) quality of the neigborhood where parent and child live

Assume that there is no effect of X on Y. But there is a direct positive effect of Z on Y.

We have a multiple regression with Y on the left hand side and X on the right side together with Z (to control for parental effects).

This implies that we condition on parent education; say de focus/condition on parental education in 70-80th percentile. 

Then we seem to see a negative "effect" (correlation) of grand parents on grand children (while the actual effect equals 0).

Why does this happen? First, consider families in bad neighborhoods. For the parents in this neighborhood to be in the 70-80th percentile, these parents had highly educated parents (that is, highly educated grandparents in our story). Parents from good neighborhoods had on average badly educated parents (grand parents in our story).

Since we assume that grandparents (X) have no effect on grandchildren educational achievements (Y) and we assume that we cannot observe neighborhoods (U), we find a negative correlation between X and Y.



#+BEGIN_SRC ditaa :file Collider2.png

+--------------+     +--------------+     +--------------+   
|              |     |              |     |              |
|  grandparent |---->|    parent    |<----|  grandchild  | 
|  education   |     |   education  |     |  education   |
|              |     |              |     |              |
+--------------+     +--------------+     +--------------+
                                  ^          ^
                                  |          |
                                  |          |
                                +--------------+
                                |              |
                                | neighborhood |
                                |   effects    |
                                |              |
                                +--------------+
                                
#+END_SRC

#+RESULTS:
[[file:Collider2.png]]



#+BEGIN_SRC ipython
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
#+END_SRC

#+RESULTS:
:results:
# Out[2]:
:end:

#+BEGIN_SRC ipython
N_observations = 10000
alpha = 0.5
X = np.random.normal(0,1,size = N_observations)
U = np.random.normal(0,1,size = N_observations)
Z = X + alpha*U+np.random.normal(0,0.5,size = N_observations)
Y = Z + alpha*U+np.random.normal(0,0.5,size = N_observations)

df = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z})
print(df.head())
#+END_SRC

#+RESULTS:
:results:
# Out[15]:
# output
          X         Y         Z
0 -0.125469  0.785660  0.082831
1  0.785645  0.069398  0.873804
2  1.295031  2.264327  1.384449
3 -0.432399  0.006915 -0.124732
4  0.333493  0.365717  0.737654

:end:

#+BEGIN_SRC ipython
results = smf.ols('Y ~ X + Z', data=df).fit()
print(results.summary())
#+END_SRC

#+RESULTS:
:results:
# Out[16]:
# output
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.852
Model:                            OLS   Adj. R-squared:                  0.852
Method:                 Least Squares   F-statistic:                 2.879e+04
Date:                Thu, 11 Apr 2019   Prob (F-statistic):               0.00
Time:                        09:15:01   Log-Likelihood:                -9270.5
No. Observations:               10000   AIC:                         1.855e+04
Df Residuals:                    9997   BIC:                         1.857e+04
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0016      0.006     -0.253      0.800      -0.014       0.010
X             -0.5148      0.011    -48.739      0.000      -0.536      -0.494
Z              1.5146      0.009    175.126      0.000       1.498       1.532
==============================================================================
Omnibus:                        0.654   Durbin-Watson:                   1.993
Prob(Omnibus):                  0.721   Jarque-Bera (JB):                0.634
Skew:                          -0.018   Prob(JB):                        0.728
Kurtosis:                       3.012   Cond. No.                         3.23
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

:end:

#+BEGIN_SRC ipython
results2 = smf.ols('Y ~ X', data=df).fit()
print(results2.summary())

#+END_SRC

#+RESULTS:
:results:
# Out[17]:
# output
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.398
Model:                            OLS   Adj. R-squared:                  0.398
Method:                 Least Squares   F-statistic:                     6616.
Date:                Thu, 11 Apr 2019   Prob (F-statistic):               0.00
Time:                        09:15:19   Log-Likelihood:                -16286.
No. Observations:               10000   AIC:                         3.258e+04
Df Residuals:                    9998   BIC:                         3.259e+04
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0014      0.012      0.113      0.910      -0.023       0.026
X              0.9976      0.012     81.338      0.000       0.974       1.022
==============================================================================
Omnibus:                        3.960   Durbin-Watson:                   1.988
Prob(Omnibus):                  0.138   Jarque-Bera (JB):                4.193
Skew:                           0.003   Prob(JB):                        0.123
Kurtosis:                       3.100   Cond. No.                         1.01
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

:end:



* Descendant

