{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/janboone/datascience_course/blob/master/Statistical_Hacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VDI4r3BYfGH"
   },
   "source": [
    "# Why statistical hacking?\n",
    "\n",
    "In order to get started on the \"big data\" part of this course, it is important that you have a good intuition about a number of statistical results that you have seen before in your statistics and econometrics courses. We go over these concepts by simulating our own data. This serves two goals:\n",
    "\n",
    "* you fully understand where the data comes from (you generated it yourself)\n",
    "* we train your programming skills in generating the data.\n",
    "\n",
    "Being able to generate your data using python simulations is a first step in understanding how to deal with \"real\" data. \n",
    "\n",
    "Based on [this lecture](https://www.youtube.com/watch?v=Iq9DzN6mvYA) and [this one.](https://www.youtube.com/watch?v=VR52vSbHBAk)\n",
    "\n",
    "If you like, there is also [free book](https://greenteapress.com/wp/think-stats-2e/) on this topic using python.\n",
    "\n",
    "We are going to discuss the following topics. First, as you probably know, estimated parameters have a distribution. To illustrate, you may recall that some statistics have a t-distribution. When you move into machine learning, the models get so complicated that there are no analytical results on the distributions of estimated coefficients. So how do you get a sense of uncertainty in that case? You simulate the distribution. Below we show this with examples where you actually know (or should know) what the relevant distributions are.\n",
    "\n",
    "Sometimes you only have a sample and no knowledge about the underlying model. Then you cannot simulate the distribution. But you can use the sample to learn something about the uncertainty underlying your parameters. This is called bootstrapping.\n",
    "\n",
    "In Economics we are not only interested in predicting variables (like economic growth or stock prices) but we also want to influence them through policy interventions. For this we need to know causal links between variables; not just correlations. Although many people have the intuition that it is better to control for more variables in  a regression, this intuition is actually not correct. By controlling for some variables, you actually mess up your interpretation of an effect in a regression.\n",
    "\n",
    "Finally, with \"big data\" it is very tempting to use a lot of variables in your models. You have got all these variables in your \"big data set\" so why not use them? This brings us to the concepts of over- and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VDI4r3BYfGH"
   },
   "source": [
    "# Loading packages\n",
    "\n",
    "To generate the data, we are going to use [tensorflow](https://www.tensorflow.org/). If you are running this in colab, install `tensorflow` and `linearmodels` by un-commenting the next cell.\n",
    "\n",
    "Run the other cell to import all the packages that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44376,
     "status": "ok",
     "timestamp": 1558779208617,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "mx-GwyO5avzp",
    "outputId": "2291d5d8-d323-41b1-926b-132c8c511cec"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.0.0-alpha0\n",
    "#!pip install linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEGgjmMeX6U3"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "import statsmodels.api as sm # check the error that cannot import name 'factorial' in from scipy.misc import factorial\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import altair as alt\n",
    "from linearmodels.iv import IV2SLS\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "#alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQO97pjfeQOK"
   },
   "source": [
    "# Distributions of an estimator\n",
    "\n",
    "Let's start simple and consider a uniform distribution on $[0,1]$. We take a sample of size `sample_size` and calculate the mean $m$ of this sample. This variable $m$ is a sample statistic. We want to understand the properties of this sample statistic.\n",
    "\n",
    "In most courses that you have done, you will have used statistical theory here. E.g. what is the distribution of $m$?\n",
    "\n",
    "For simple models, like calculating the mean of a sample or doing an OLS regression, there is theory that describes what the distributions are of the mean or an estimated slope parameter. However, with big data techniques, you are going to use models where such theoretical results do not exist. So how do you figure out then what is happening?\n",
    "\n",
    "This is where the hacking comes in. The simple idea is: we program the statistical problem and then run it \"lots of times\", say 10,000 times. This gives us a distribution.\n",
    "\n",
    "## distribution of a sample average and sample standard deviation\n",
    "\n",
    "For the example of $m$, we do this in the next code cell.\n",
    "\n",
    "We draw `N_simulations` times a sample of `sample_size` and put this in a matrix (tensor) of size `N_simulations` by `sample_size`. Then we take the mean within a sample. Put differently, we take the mean over the second dimension (\"columns\") of the matrix. As python starts counting from 0, this means we take the mean over `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6palYMJded3Z"
   },
   "outputs": [],
   "source": [
    "N_simulations = 10000\n",
    "sample_size = 10\n",
    "simulated_data = np.mean(tf.random.uniform([N_simulations,sample_size],0,1),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHe1np30bARr"
   },
   "source": [
    "The code above calculated `N_simulations` averages (of samples of size `sample_size`). \n",
    "\n",
    "**Question** Plot a histogram of `simulated_data` using `matplotlib`.\n",
    "\n",
    "**Question** When considering this distribution, which \"theorem\" is at work here?\n",
    "\n",
    "**Exercise** Play around with `sample_size` to see what the effect is of different values for this variable.\n",
    "\n",
    "**Exercise** Calculate the standard deviation of $m$; how does this depend on `sample_size`? Can you plot the standard deviation of $m$ against sample size? Which functional form is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 1275,
     "status": "ok",
     "timestamp": 1556181123319,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "oFLg0pyRf-rt",
    "nbgrader": {
     "checksum": "31ce2ccd05be64e434d19a4101d08333",
     "grade": true,
     "grade_id": "cell-c35a8216d1291f17",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "7173ccdc-8f1c-415b-b61d-b0bbe1d68dea"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "32d4ab969be5de1e8c66f6d3bd176cd7",
     "grade": true,
     "grade_id": "cell-2288157c46091176",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a90c39d4da02cc846bebf6e3baf809a4",
     "grade": true,
     "grade_id": "cell-dcce96dcaeb0093f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we considered the distribution of the sample mean $m$. But any parameter has a distribution; e.g. also the standard deviation of the sample $s$ has a distribution. Note that the standard deviation of the sample $s$ is **not** the same as the standard error $\\sigma/\\sqrt{n}$ which is the standard deviation of $m$'s distribution. Do not worry if you find the last sentence confusing in the beginning...\n",
    "\n",
    "**Question** Plot the distribution of $s$. What is the probability that $s<0.28$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b77bfcc06011426c3aa00a57b84bed6c",
     "grade": true,
     "grade_id": "cell-bbd11ce0c350942c",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1yOkLQ-ojtC"
   },
   "source": [
    "## distribution of a slope\n",
    "\n",
    "If we run a simple OLS (ordinary least squares) linear regression, we find the constant and the slope of this line. These parameters have also distributions! We can plot these distributions as well.\n",
    "\n",
    "We first generate the data with which we want to estimate a regression line.\n",
    "\n",
    "**Question** Explain the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dbanu7HXcD9q"
   },
   "outputs": [],
   "source": [
    "N_simulations = 10000\n",
    "sample_size = 20\n",
    "slope = 0.5\n",
    "constant = 1.0\n",
    "noise = 0.1\n",
    "simulated_x = tf.random.normal([N_simulations,sample_size])\n",
    "simulated_y = constant + slope * simulated_x + noise*tf.random.normal([N_simulations,sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a vector `constants` and a vector `slopes`. For each of our simulations we add the resulting constant and slope to their resp. vectors.\n",
    "\n",
    "Note that the method `.numpy()` turns the tensorflow tensors into numpy arrays which are easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmNG9Mw8fwWT"
   },
   "outputs": [],
   "source": [
    "constants = []\n",
    "slopes = []\n",
    "\n",
    "for i in range(N_simulations):\n",
    "    model = sm.OLS(simulated_y[i,:].numpy(), sm.add_constant(simulated_x[i,:].numpy()))\n",
    "    results = model.fit().params\n",
    "    constants.append(results[0])\n",
    "    slopes.append(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbuZpg9Z1MMp"
   },
   "source": [
    "**Question** Plot the distribution of slopes and the distribution of constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 6835,
     "status": "ok",
     "timestamp": 1556181140121,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "GYgc5gxZgAmx",
    "nbgrader": {
     "checksum": "e746ee02651b0ee7e66147ecf45ad11e",
     "grade": true,
     "grade_id": "cell-32124b40f04fcbbb",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "9f74c1ad-2b14-473e-b842-d434c69aae57"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1556181152549,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "vrCBf-h5hX9D",
    "nbgrader": {
     "checksum": "1180cf32e8d2d801150e29c97228bf36",
     "grade": true,
     "grade_id": "cell-a15ff0e5de2ccdec",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "fe6ec456-0d47-40e7-d5c1-6e096afdc288"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXZHvqN71Y1k"
   },
   "source": [
    "Instead of plotting the slopes and constants separately, we can plot the lines that are induced by these distributions of the constant and the slope.\n",
    "\n",
    "**Exercise** Plot in $(x,y)$ space, the distribution of lines that follow from these distributions of slopes and constants.\n",
    "\n",
    "**Question** For which values of $x$ is the uncertainty about $y$ the biggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 4074,
     "status": "ok",
     "timestamp": 1556181164969,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "roQh2rG0ihye",
    "nbgrader": {
     "checksum": "1139bf7567a43e4c50295dde4873be76",
     "grade": true,
     "grade_id": "cell-b9682c658983df70",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "6731be15-f494-4423-f7b6-8781c530a827"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWAjo1Clo1Hi"
   },
   "source": [
    "# Bootstrapping\n",
    "\n",
    "Sometimes we do not know what the underlying model is that generated the data. We only have the sample that we observed.  How do we proceed in this case if we want to test properties of the data? Here we can use bootstrapping.\n",
    "\n",
    "\n",
    "\n",
    "Suppose that we have two populations (e.g. men vs women; or in the test of a new drug: patients that got the treatment and patients that received the placebo). Denote these two samples by $A$ and $B$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1556181181292,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "L5dfK9IXsRb7",
    "outputId": "c91dba2c-ab20-4b93-e9c2-b026c009f2eb"
   },
   "outputs": [],
   "source": [
    "sample_size = 50\n",
    "delta = 0.95\n",
    "A = 10+tf.random.normal([sample_size])\n",
    "B = A*delta\n",
    "plt.hist(A,label='sample A')\n",
    "plt.hist(B,alpha=0.3,label='sample B')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrA6njSSteju"
   },
   "source": [
    "**Question** Show that the average of $A$ is higher than the average of $B$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1556181184976,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "K9B8sNaltB1D",
    "nbgrader": {
     "checksum": "ead02dd9a6bf232d08b8482227a7c4c2",
     "grade": true,
     "grade_id": "cell-d72270b078e90ff6",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "cac9ed0b-b430-4b9e-f16c-d0945a137086"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJKjGsGit0x_"
   },
   "source": [
    "But is this difference in means a significant effect (forget for a second that you saw the code generating the data)?\n",
    "\n",
    "In your statistics class you have seen a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) to figure out whether this difference is significant or not. But we are going to use simulations to establish this.\n",
    "\n",
    "The null hypothesis that we want to test is: samples $A$ and $B$ are drawn from the same distribution.\n",
    "\n",
    "If this is true, than we can simply combine the $A$ and $B$ observations, reshuffle them and calculate the difference. We do this 10,000 times and get a distribution of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h27aRxCOwPYu"
   },
   "outputs": [],
   "source": [
    "AB = tf.concat([A,B],axis=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MBw5Li8w13q"
   },
   "outputs": [],
   "source": [
    "differences = []\n",
    "\n",
    "for i in range(10000):\n",
    "  np.random.shuffle(AB)\n",
    "  differences.append(np.mean(AB[:sample_size])-np.mean(AB[sample_size:]))\n",
    "                             \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1556181222069,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "dIBLGlk_yeJ0",
    "outputId": "bfabd576-107c-48c3-a931-eae934149850"
   },
   "outputs": [],
   "source": [
    "plt.hist(differences)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1556181232398,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "NqBohKU1ygyI",
    "outputId": "d122f36a-320c-473f-f9f1-7aac4d58941a"
   },
   "outputs": [],
   "source": [
    "sum(differences>observed_difference)/len(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWPW6HI2HcVm"
   },
   "source": [
    "# Doing your own OLS\n",
    "\n",
    "There exist many packages that can do OLS for you. But it is illustrative, to program your own OLS estimator, because then you can see what it does.\n",
    "\n",
    "We start really simple and through the lecture we are going to make the estimator more sophisticated.\n",
    "\n",
    "For OLS, we could just substitute the solution using matrix algebra: $\\beta = (x^Tx)^{-1}x^Ty$. But we want to use the optimization problem itself so that we can add things to it later on.\n",
    "\n",
    "First, we are going to generate our own data.\n",
    "\n",
    "Then we define a loss function (mean squared error) and minimize the loss by choosing a constant (`w[0]`) and slope (`w[1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1556358574664,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "BWhR_jcFhmbe",
    "outputId": "b55ec367-f27b-4d0c-e5b4-f7f8fa83f502"
   },
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "slope = -3.0\n",
    "constant = 1.0\n",
    "x = tf.random.normal([sample_size])\n",
    "y = slope*x+constant+3*tf.random.normal([sample_size])\n",
    "\n",
    "def loss(w):\n",
    "    return np.sum((w[0]+w[1]*x-y)**2)/len(y)\n",
    "\n",
    "optimize.fmin(loss, [0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are ridge and lasso regressions?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ridge(w,λ):\n",
    "    return np.sum((w[0]+w[1]*x-y)**2)/len(y)+λ*(w[0]**2+w[1]**2)\n",
    "\n",
    "optimize.fmin(lambda w: loss_ridge(w,1), [0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_lasso(w,λ):\n",
    "    return np.sum((w[0]+w[1]*x-y)**2)/len(y)+λ*(np.abs(w[0])+np.abs(w[1]))\n",
    "\n",
    "optimize.fmin(lambda w: loss_lasso(w,1), [0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Psozyj3XcdA8"
   },
   "source": [
    "# Causality\n",
    "\n",
    "The data we generate is inspired by [Richard McElreath\n",
    "'s lecture 6](https://www.youtube.com/watch?v=l_7yIUqWBmE). \n",
    "\n",
    "As you know, multiple regression is about correlations, not about causality. When looking at the results of a multiple regression, there are a number of mistakes that you can make. Here we are going to look at four of these mistakes:\n",
    "\n",
    "* the fork\n",
    "* the pipe\n",
    "* the collider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTxJRtkedmgi"
   },
   "source": [
    "## The fork\n",
    "\n",
    "This is the situation where there is no relation between the variables $X$ and $Y$. However, both are influenced by a third variable $Z$. \n",
    "\n",
    "\n",
    "\n",
    "![fork](./Fork.png)\n",
    "\n",
    "\n",
    "Doing a regression from $X$ on $Y$ seems to suggest a causal effect of $X$ on $Y$. This could misleadingly convince people that a policy that affects $X$ directly will also affect $Y$.\n",
    "\n",
    "However, using both $X$ and $Z$ as explanatory variables in the regression will show that there is no significant effect from $X$ on $Y$.\n",
    "\n",
    "Hence, with a fork it is important to think about possible variables $Z$ and include them in your regression.\n",
    "\n",
    "**Question** Generate data for the variables $X,Y,Z$ such that there is a fork\n",
    "\n",
    "**Question** Put this data in a pandas dataframe.\n",
    "\n",
    "With the dataframe, it is easy to run the regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qs3nhFBHdnKF"
   },
   "source": [
    "## The pipe\n",
    "\n",
    "![pipe](./Pipe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42ZPCNqMdm2l"
   },
   "source": [
    "## The collider\n",
    "\n",
    "Many people have the intuition that adding a control variable to a regression (especially when it turns out to be signficant) is a good idea. However, this intuition is wrong if it is applied without thinking.\n",
    "\n",
    "We use here the education example by Richard McElreath to illustrate this point. Consider the educational achievements of three generations in one family: the grandparent, the parent and the (grand)child. We are interested in the effect of the grandparent's education on the educational achievement of the grandchild. \n",
    "\n",
    "Let's denote the child's educational achievement by $Y$ (the variable we are interested in) and the grandparent's education $X$. We want to understand the effect of $x$ on $Y$. Clearly there is the following path: more educated grandparent leads to more educated parent which, in turn, leads to more educated grandchild. The thing we are interested in is whether there is a direct effect from the grandparent on the child, *controlling* for the parent's education.\n",
    "\n",
    "![collider](./Collider.png)\n",
    "\n",
    "We know that the parent's education has a positive effect on the child's achievement. A more educated parent will tend to stimulate their children more, can help with homework etc. In the data that we generate below, we assume that $Z = X + ...$ and $Y = Z + ...$. That is, we assume that the parent's education feeds one-to-one into the child's education. \n",
    "\n",
    "Is it the case that on top of this effect there is a separate grandparent effect? E.g. because the grandparent is babysitting and a more educated grandparent reads Hamlet with the 5 year old grandchild. That is fun and can boost the child's educational achievement.\n",
    "\n",
    "Another important component of education is the neighborhood where the child grows up. We denote this variable $U$. In the code we assume that the parent and child grow up in the same neighborhood (have the same $U$ effect).  $U$ is drawn from a standard normal distribution and the effect on education is given through a multiplication by the factor `alpha`.\n",
    "\n",
    "In addition to this, we assume that there is a random component as well which differs between parent and child.\n",
    "\n",
    "![collider](./Collider2.png)\n",
    "\n",
    "The code below, generates this data and a dataframe `df` that stores this data.\n",
    "\n",
    "Look carefully at the code: is there a direct effect from the grandparent to the grandchild's educational achievement?\n",
    "\n",
    "We generate two dataframes: `df` is the one that is observed and used in the analysis by the researcher. \n",
    "\n",
    "We will use `df_2` to illustrate why one can get incorrect results from analyzing `df`. The problem is that the researcher does not have access to `df_2` but we can analyze `df_2` for \"educational purposes\" to illustrate why problems arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1556188665098,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "YbGfWVekX-8G",
    "outputId": "543372db-4639-456d-c1d1-d192de042d09"
   },
   "outputs": [],
   "source": [
    "N_observations = 5000\n",
    "alpha = 0.5\n",
    "X = tf.random.normal([N_observations])\n",
    "U = tf.random.normal([N_observations])\n",
    "Z = X + alpha*U+0.4*tf.random.normal([N_observations])\n",
    "Y = Z + alpha*U+0.4*tf.random.normal([N_observations])\n",
    "\n",
    "df = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z})\n",
    "df_2 = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z, 'U':U})\n",
    "print(df.head())\n",
    "print(df_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2ED-R_JNHfD"
   },
   "source": [
    "To find the (direct) effect of the grandparent ($X$) and the grandchild's educational achievement ($Y$) conditional on the parent ($Z$), we use multiple regression where both $x$ and $Z$ are used as explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1556188667363,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "st0jhpiEYD3z",
    "outputId": "c9ffbeb6-4dbd-46a7-bbbe-735b9f1977ed"
   },
   "outputs": [],
   "source": [
    "results = smf.ols('Y ~ X + Z', data=df).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlVQK6LCNroW"
   },
   "source": [
    "The effect of the grandparent's educational achievement ($X$) on the child's achievement is negative and significant. Where does that come from? How is this even possible?\n",
    "\n",
    "If we consider the direct effect of $X$ on $Y$ it is --as expected-- positive and significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1556188668741,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "7WNHOCh1YGWt",
    "outputId": "8804e459-cd69-4c38-fdd6-32a972e898ad"
   },
   "outputs": [],
   "source": [
    "results2 = smf.ols('Y ~ X', data=df).fit()\n",
    "print(results2.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILo37WUppWMw"
   },
   "source": [
    "To see what is happening here in a graph, let's think about what it means to \"control for parental education $Z$\". \n",
    "\n",
    "To visualize what happens, we condition on $Z$ by focusing on a narrow band of $Z$ observations. In other words, we condition on $Z$ by focusing on a subsample where the $Z$ values are (almost) the same. We do this by creating a column `selected` in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zzc8Qj5djoaI"
   },
   "outputs": [],
   "source": [
    "df['selected'] = np.abs(df.Z)<0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1556188670972,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "ythl_bmoiTVc",
    "outputId": "4e49ce97-594b-4cdc-85ac-9059901ff3d3"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMwDuCmh1LCt"
   },
   "source": [
    "Feel free to make this plot using `matplotlib`. For fun, we use `altair` here. \n",
    "\n",
    "The blue circles are all the data, the orange ones are the data with (almost) the same $Z$.\n",
    "\n",
    "**Question** Give the intuition why there is a negative correlation between $X$ and $Y$ with the orange dots. [hint: what do you know about orange dots with low $X$ and about orange dots with high $X$?]\n",
    "\n",
    "If you cannot answer this question yet, consider the following interactive graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1234,
     "status": "ok",
     "timestamp": 1556188713665,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "1FocqUANiWT7",
    "outputId": "636e6b18-f944-4852-f1af-eaf4b3118abb"
   },
   "outputs": [],
   "source": [
    "\n",
    "chart = alt.Chart(df).mark_point().encode(\n",
    "  x='X',\n",
    "  y='Y',\n",
    "  color='selected'\n",
    ").interactive()\n",
    "\n",
    "chart.save('Chart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<iframe width=\"840\" height=\"400\" src=\"./Chart.html\" frameborder=\"0\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOp8m46p14Au"
   },
   "source": [
    "To better understand what is happening here, we are going to use `df_2`. In particular, in the combined figure below, we again provide a scatter plot of `X` vs `Y`. Now the color indicates the level of `Z` associated with the observation; darker colors indicate higher values of `Z`. The size of the point (circle) indicates the level of `U` (that the researcher cannot see in `df`). \n",
    "\n",
    "To condition on `Z` (as we do in a multiple regression), we can drag a rectangle in the bottom histogram. Move this histogram around and see what happens. In particular:\n",
    "* how does `Z` vary in the figure? E.g. where are the high values of `Z`? What is the interpretation of this?\n",
    "* for a selection of `Z` in the bottom histogram, what is the relation between `X` and `Y`?\n",
    "* for this selection, how does `U` vary in the figure? What is the interpretation?\n",
    "\n",
    "\n",
    "**Question** Give the intuition why there is a negative correlation between `X` and `Y` for a selection of `Z`; that is, conditional on `Z`.\n",
    "\n",
    "**Exercise** What is the risk of saying: I have run a regression and controlled for all effects that I had variables for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1556195913155,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "VC8-77Rf12u5",
    "outputId": "96cd0c7c-22ba-4fc4-f98c-513d4b8e2f23"
   },
   "outputs": [],
   "source": [
    "interval = alt.selection_interval(encodings=['y'])\n",
    "\n",
    "fig = alt.Chart(df_2).mark_point().encode(\n",
    "  x='X',\n",
    "  y='Y',\n",
    "  color=alt.condition(interval, 'Z', alt.value('lightgreen')),\n",
    "  size = 'U'\n",
    ").properties(\n",
    "    selection=interval\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "hist = alt.Chart(df_2).mark_bar().encode(\n",
    "    x='count()',\n",
    "    y=alt.Y('Z',bin=True),\n",
    "    color=alt.condition(interval, 'Z', alt.value('lightgrey'))\n",
    ").properties(\n",
    "    selection=interval\n",
    "\n",
    ")\n",
    "\n",
    "chart2 = fig & hist\n",
    "\n",
    "chart2.save('Chart2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<iframe width=\"840\" height=\"800\" src=\"./Chart2.html\" frameborder=\"0\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jyve80prPAB2"
   },
   "source": [
    "# Some background on tensors\n",
    "\n",
    "We will have a technical interlude to explain what tensors are and what you can do with them. We describe some functions that can be applied to tensors. To motivate these functions, we will build our first neural network. The point here is not so much to explain how neural networks works but more to show you that the functions we consider are going to be useful later on when we will try to understand neural networks more deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOGCDKigifED"
   },
   "source": [
    "## Tensors\n",
    "\n",
    "In most of the empirical analyses that you have done, an observation is a one dimensional vector. E.g. you have data on inflation, unemployment, gdp growth for country-year combinations. Then for the UK in 2000, our data  consists of the one dimensional vector `[inflation, unemployment, gdp growth]`. If you have 100 observations like this, you can represent them in a matrix. Each row is an observation and the columns will be `[coutry, year, inflation, unemployment, gdp growth]`. Hence we have a two dimensional dataset with 100 rows and 5 columns.\n",
    "\n",
    "In big data, there are usually higher dimension observations. One of the \"classic\" datasets in machine learning is [the MNIST dataset.](https://en.wikipedia.org/wiki/MNIST_database) This dataset consists of handwritten numbers and their corresponding label (e.g. when the handwritten number is 5, the label for this image `5`). We will see an example shortly.\n",
    "\n",
    "The point is that a handwritten number is a two dimensional observation. To illustrate this, consider the 5th image from the training dataset (python starts numbering at 0); we plot the handwritten image (which is 2 dimensional) and print its label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1556370513629,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "SmEybeeliYi8",
    "outputId": "813f060a-15c5-4264-88cd-0a38cffb5c7d"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1556370516249,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "8SSJCxWMkQe6",
    "outputId": "25b3b30f-7ad0-44bd-f7fe-e4c1e133af55"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_images[4],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(train_labels[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4L5zmRgqMJo"
   },
   "source": [
    "Indeed, the handwritten number is 9, which equals its label.\n",
    "\n",
    "What are the dimensions for this dataset. Let's use `shape` to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1556282456035,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "WOg2L1fAkhZv",
    "outputId": "1ad602f3-b610-452f-a895-a7d2195a8605"
   },
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oj8-DKccqmca"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1556282548428,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "gXigpmrjqbW7",
    "outputId": "82312993-9953-49d6-c469-bd592ff2e944"
   },
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq8zXvScr19o"
   },
   "source": [
    "## Tensors in numpy\n",
    "\n",
    "Well known machine learning backends are Theano and Tensorflow. For reasons that we do not worry about here, these are not immediately straightforward to work with. But we can play around with dimensions in Numpy as well. Properties that we can use in Numpy, like broadcasting, can be used in Theano and Tensorflow as well.\n",
    "\n",
    "As Numpy is \"more direct\" to work with, we will practice this with numpy.\n",
    "\n",
    "First, we create the vector `x` with 100 random numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HmqgKGEqx6c"
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(0,1,size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yd-M7PXttRW_"
   },
   "source": [
    "We can check what the shape is of this vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1556283042608,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "1tqdba21soaO",
    "outputId": "5eec91a3-50ad-4ac4-b1e4-87acddd5ed58"
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48L7R9QvtXaF"
   },
   "source": [
    "and the vector is one-dimensional from a tensor point of view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1556283118592,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "ZHoMyyxhspAB",
    "outputId": "1998c7d9-72a8-4988-9d84-4ff7ea900936"
   },
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDNJKPz0thFs"
   },
   "source": [
    "**No not read this if you get easily confused:** If you would plot the vector `x` it would be a *vector* in a 100-dimensional space. Hence it is a 100 dimensional vector, but a 1 dimensional tensor.\n",
    "\n",
    "\n",
    "Let us know consider a two dimensional (2D) tensor. This is what we usually call a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wG27xM_s9GN"
   },
   "outputs": [],
   "source": [
    "x2 = x.reshape(25,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1556283467537,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "CkSfmwM_uNpB",
    "outputId": "6451e1d3-7504-45cf-c488-0fa2c262f5af"
   },
   "outputs": [],
   "source": [
    "x2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1556283472983,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "TMLeiOEFuSSc",
    "outputId": "90b6db0b-d1f1-44ca-d05b-83694d2752bd"
   },
   "outputs": [],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdqLDxM9uWxN"
   },
   "source": [
    "In matrix terminology we would say that `x2` has 25 rows and 4 columns.\n",
    "\n",
    "A 3D tensor is then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "so_DNWlEuTow"
   },
   "outputs": [],
   "source": [
    "x3 = x.reshape(4,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1556283585287,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Hz1MEK65utU2",
    "outputId": "1515ae6b-3194-4155-f664-73021d561656"
   },
   "outputs": [],
   "source": [
    "x3.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1556283589831,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "aRBMCshIuvEP",
    "outputId": "020521c2-71c8-4414-949d-8bf9f18866d5"
   },
   "outputs": [],
   "source": [
    "x3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1eB-x0Lu0R7"
   },
   "source": [
    "One way to think about this is as four $5*5$ images.\n",
    "\n",
    "The numpy representation of this is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1388,
     "status": "ok",
     "timestamp": 1556283635679,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "e4dvF-EiuwKo",
    "outputId": "3f4e828f-e2d3-41c7-ee87-ca3fd53106ff"
   },
   "outputs": [],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6STQAVi9vhlt"
   },
   "source": [
    "we can also add a dimension to a vector without adding or re-arranging data. For this we use `np.newaxis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhEGTMQ3u7JC"
   },
   "outputs": [],
   "source": [
    "x4 = x3[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1556283857417,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "paLCkt1svwQ1",
    "outputId": "8aa3403f-86f6-4709-8220-dc6ebbba9d80"
   },
   "outputs": [],
   "source": [
    "x4.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1556283863264,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "hLbKHsMIvxfw",
    "outputId": "c9d359fd-f43f-4c28-b7f0-819749142dca"
   },
   "outputs": [],
   "source": [
    "x4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUJF_kDIv6jg"
   },
   "source": [
    "Now you may wonder why this is a useful thing to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXi6_Jupv6rz"
   },
   "source": [
    "## Broadcasting\n",
    "\n",
    "Suppose you have estimated a fixed effect model. In your data there are i = 0,1,...,9 individuals and t = 0,1,2 periods. The vector `I` consists of 10 individual fixed effects and the vector `T` has 3 time fixed effects. Hence, our prediction for indiv. `i` in time period `t` (ignoring other explanatory variables) is $y_{it}=I_i+T_t$. \n",
    "\n",
    "How can we calculate the vector `y`?\n",
    "\n",
    "We first give you the answer using broadcasting and then we step back to explain what broadcasting is.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_z7TqTLvy7W"
   },
   "outputs": [],
   "source": [
    "I = np.arange(10)\n",
    "T = np.arange(0,30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1556284277690,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "-B3wpCzkxXdV",
    "outputId": "07a3d48d-a265-4b4f-94ab-39cc2e666412"
   },
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1556284280654,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Yo8-de_3xYGS",
    "outputId": "83819535-3f13-4a36-8a8f-39eac2c14b25"
   },
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAOnRrTJxbaM"
   },
   "source": [
    "Because of the simplistic numbers that we have chosen, it is easy to see that e.g. $y_{31} = 13$ with $i=3$ and $t=1$. So we can check whether our trick works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eua1v545xY1O"
   },
   "outputs": [],
   "source": [
    "y = I[:,np.newaxis]+T[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1556284519887,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "J_V75rdwyF-u",
    "outputId": "9d1c0348-702b-4107-f86c-639b7e053dbd"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1556284570652,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "bPxpgAr_yGcR",
    "outputId": "88a9ef5c-984f-4da9-a12d-b8b39a622340"
   },
   "outputs": [],
   "source": [
    "I[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1556284579224,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "bhOoGx2wyfoW",
    "outputId": "3f0b4c56-5baa-478b-d870-e0ca4e3a2efb"
   },
   "outputs": [],
   "source": [
    "T[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw4T0tQWyi60"
   },
   "source": [
    "What is going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGrVDEWryhtm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3lIIQBPSTWZ"
   },
   "source": [
    "## Slicing and reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0rKwqdCShHm"
   },
   "source": [
    "## First neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hZyoNwm6Je2"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23166,
     "status": "ok",
     "timestamp": 1556370965688,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "lRi2WhVB6J3i",
    "outputId": "94eeb1bc-c1ad-4a6f-d1de-29afca2d5d03"
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1556370974512,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "c-0AF9lZ6J_1",
    "outputId": "6f1e7863-2b9f-4d49-8d77-246e0ae5b1d9"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1556371028160,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Zt2s64Sv6J8-",
    "outputId": "db36cba3-ed61-4fbe-daac-b019cc464995"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1556371039255,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "0fvXLL2t6Jzd",
    "outputId": "10f210bc-7eaf-4663-8c1f-422d57499f36"
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1556371056788,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "mF4obm4L8Zf8",
    "outputId": "df71cc22-2a3b-460d-d530-7e58f87b2a01"
   },
   "outputs": [],
   "source": [
    "test_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1556371087956,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Jj51sTbh8Ztc",
    "outputId": "e16c1322-1602-4435-fc59-3ac6313170b0"
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_images[0],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zShpGRrSWzU"
   },
   "source": [
    "## Functions\n",
    "\n",
    "What is `relu` and `softmax`? What does `adam` mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qIrjecpSaki"
   },
   "outputs": [],
   "source": [
    "np.maximum(z,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 488,
     "status": "error",
     "timestamp": 1556360126067,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "OpC626BQSd_6",
    "outputId": "9e4dd666-7e2c-4f75-d38f-17364af5cd34"
   },
   "outputs": [],
   "source": [
    "np.relu(10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "woigU8j1SeM-"
   },
   "outputs": [],
   "source": [
    "np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOLKHDRRSeTg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2JkGgrO3o-i"
   },
   "source": [
    "# Overfitting and underfitting\n",
    "\n",
    "\n",
    "Adding more explanatory variables to a regression cannot make the fit worse. This is easy to see: if it would become worse, you would set the coefficient on the added variables to 0. That would give the same fit as before. In real applications that may not be completely true as there is often randomness in the routines that are being used.\n",
    "\n",
    "underfitting is ...\n",
    "\n",
    "overfitting is ...\n",
    "\n",
    "\n",
    "So how can we choose what the right model is?\n",
    "\n",
    "A simple idea is cross validation. This idea is seen a lot in data science. Here we consider a simple example to illustrate cross validation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzUwTixOGAtl"
   },
   "source": [
    "## Regression in tensorflow\n",
    "\n",
    "Below we will run regressions for a number of models. To understand what happens, let us first run a simple regression to see what the syntax looks like.\n",
    "\n",
    "There are a number of python packages with which you can do a regression analysis. For example, [statsmodels](https://www.statsmodels.org/stable/index.html) has a nice syntax and by default returns things like t-values etc. If you just need to run a regression or two, than this is a great choice.\n",
    "\n",
    "But if we want to program a series of regressions, tensorflow or [scikit-learn](https://scikit-learn.org/stable/index.html) are more useful as they are easier to program.\n",
    "\n",
    "Here we use tensorflow.\n",
    "\n",
    "First, we generate some simple data and add these to a dataframe.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bqz09FygKJkz"
   },
   "outputs": [],
   "source": [
    "N_observations = 50\n",
    "train_size = 25\n",
    "x = tf.random.normal([N_observations])\n",
    "y = 3*x+5*tf.random.normal([N_observations])\n",
    "df = pd.DataFrame({'y':y,'x':x})\n",
    "df['constant'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHiMJS8zKWDn"
   },
   "outputs": [],
   "source": [
    "features = ['constant','x']\n",
    "\n",
    "y_train = df['y'][:train_size]\n",
    "y_test =  df['y'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pd7LJXjCKuxF"
   },
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, batch_size=32):\n",
    "  def input_function():\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    return ds\n",
    "  return input_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnVLN7DcLY6L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1499,
     "status": "ok",
     "timestamp": 1556276442448,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "93U1z1_DKu2b",
    "outputId": "9bc61fe9-1d3e-47aa-d4ff-3b35097f08bb"
   },
   "outputs": [],
   "source": [
    "df_train = df[features][:train_size]\n",
    "df_test = df[features][train_size:]\n",
    "NUMERIC_COLUMNS = features\n",
    "feature_columns = []\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "\n",
    "train_input_fn = make_input_fn(df_train, y_train)\n",
    "eval_input_fn = make_input_fn(df_test, y_test)\n",
    "  \n",
    "linear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "linear_est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1556276483846,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "73RgC7ZiKu6B",
    "outputId": "46c70cd1-9e72-48f0-eb34-ea839b9e3d40"
   },
   "outputs": [],
   "source": [
    "linear_est.evaluate(train_input_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKu0b_AjdHYo"
   },
   "source": [
    "## Series of regressions\n",
    "\n",
    "\n",
    "To illustrate overfitting, we construct a dataset, generating $y$ values from some $x$ vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjffTAiHWP0I"
   },
   "outputs": [],
   "source": [
    "N_observations = 50\n",
    "train_size = 25\n",
    "x = tf.random.normal([N_observations])\n",
    "y = 100*x+np.cos(x)-4*np.exp(x)+x**2+x**5-x**3+55*tf.random.normal([N_observations])\n",
    "df = pd.DataFrame({'y':y,'x':x})\n",
    "df['constant'] = 1\n",
    "df['x2'] = df.x**2\n",
    "df['x3'] = df.x**3\n",
    "df['x4'] = df.x**4\n",
    "df['x5'] = df.x**5\n",
    "df['x6'] = df.x**6\n",
    "df['x7'] = df.x**7\n",
    "df['x8'] = df.x**8\n",
    "df['x9'] = df.x**9\n",
    "df['x10'] = df.x**10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDRvPbj0_kYL"
   },
   "outputs": [],
   "source": [
    "features = ['constant','x','x2','x3','x4','x5','x6','x7','x8','x9','x10']\n",
    "\n",
    "y_train = df['y'][:train_size]\n",
    "y_test =  df['y'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylwHFWyfKu93"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOaROcECKvAv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFySuZV9EXit"
   },
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=False, batch_size=32):\n",
    "    def input_function():\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000)\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        return ds\n",
    "    return input_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "1BxJyEXFQUbi",
    "outputId": "be7000fa-ba2c-4310-dcd2-7d211ac43bb3"
   },
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, batch_size=32):\n",
    "    def input_function():\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        return ds\n",
    "    return input_function\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "number_features = []\n",
    "\n",
    "for i in range(2,len(features)+1):\n",
    "    df_train = df[features[:i]][:train_size]\n",
    "    df_test = df[features[:i]][train_size:]\n",
    "    NUMERIC_COLUMNS = features[:i]\n",
    "    feature_columns = []\n",
    "    for feature_name in NUMERIC_COLUMNS:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "      \n",
    "    train_input_fn = make_input_fn(df_train, y_train)\n",
    "    eval_input_fn = make_input_fn(df_test, y_test)\n",
    "  \n",
    "    linear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "    linear_est.train(train_input_fn)\n",
    "  \n",
    "    print(i)\n",
    "    number_features.append(i)\n",
    "    train_score.append(linear_est.evaluate(train_input_fn)['average_loss'])\n",
    "    test_score.append(linear_est.evaluate(eval_input_fn)['average_loss'])\n",
    "  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "nkk_YtpiQUS8",
    "outputId": "381d34f0-82dd-45ff-8557-de1102ac6ce5"
   },
   "outputs": [],
   "source": [
    "outcome = pd.DataFrame({'x_i':number_features, 'train_score': train_score, 'test_score': test_score})  \n",
    "outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKuRHwBjMJCS"
   },
   "source": [
    "The following graph shows that adding more terms $x^a$ always helps to get a better fit on the training data. The loss measure falls as we add more terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "16ASFb3Y2vom",
    "outputId": "992ae3b3-30c1-451f-f053-df658a65cd63"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(2,12),outcome.train_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T50VYSfoMS14"
   },
   "source": [
    "But this is not the case with the test data. We fitted the model on the training data. Now we consider how well the model predicts \"out-of-sample\" on the test data. Adding 6 terms helps to predict on the test data. However, with more terms added to the model, the model starts to pick up features that are specific to our sample in the training data. These features are nor general to the underlying process. Hence, our loss increases on the test data.\n",
    "\n",
    "By adding more than 6 terms, we start to overfit in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "O6oBuSHO0-vG",
    "outputId": "5625a9bd-436a-45bc-da91-7202ec73a144"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(2,12),np.log(outcome.test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lbmsd3sZiROP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6w3x-iFUM-XW"
   },
   "source": [
    "# OLS using Tensorflow\n",
    "\n",
    "Using more functionality from Tensorflow to program our OLS.\n",
    "\n",
    "This allows us to see some building blocks that we will need below to program our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f88ttpvzN6zB"
   },
   "source": [
    "# Cross validation\n",
    "\n",
    "\n",
    "k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_wrL0zKOM9y"
   },
   "source": [
    "# Neural network\n",
    "\n",
    "To get a first sense of how a neural network works, we look at two simple python programs that implement these algorithms. For this, we use the website from the book [machine learning: an algorithmic perspective](http://homepages.ecs.vuw.ac.nz/~marslast/MLbook.html).\n",
    "\n",
    "Below you find the code for the files [pcn_logic_eg.py](http://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch3/pcn_logic_eg.py) and [mlp.py](http://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch4/mlp.py); which we adapted for python 3 by adjusting the print statements (should be `print()` in python 3).\n",
    "\n",
    "By looking at the underlying code we get an idea how neural networks work. Then we create a neural network using tensorflow 2.0. This part is based on a [tensorflow 2.0 tutorial](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/estimators/premade_estimators.ipynb).\n",
    "\n",
    "The tensorflow syntax is very similar to [Keras](https://keras.io/). There is a [datacamp course](https://www.datacamp.com/courses/deep-learning-in-python) on deep learning with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Z6Wtb_jisbA"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "QUyRGn9riopB"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnRNqPCr5Ujq"
   },
   "source": [
    "First we install/import the libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42840,
     "status": "ok",
     "timestamp": 1559229836258,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "jPo5bQwndr9P",
    "outputId": "9091d80d-6082-4ff3-f2dc-5a85cddd6aa3"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMybA1huZ3n9"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4I1uBJPHV5rJ"
   },
   "source": [
    "## Perceptron\n",
    "\n",
    "Next we copy from Chapter 3 of Machine Learning: An Algorithmic Perspective, the \"perceptron\". The code is in the next cell. Try to understand the python.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The main part of the algorithm is in the function (method, actually, but do not worry about this) `pcntrain`; the line\n",
    "\n",
    "`self.weights -= eta*np.dot(np.transpose(inputs),self.activations-targets)`\n",
    "\n",
    "determines how the weights in the network are updated. This optimization step is discussed in [this chapter of the datacamp course.](https://campus.datacamp.com/courses/deep-learning-in-python/optimizing-a-neural-network-with-backward-propagation?ex=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaWTqADydTqn"
   },
   "outputs": [],
   "source": [
    "# Code from Chapter 3 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "\n",
    "\n",
    "class pcn:\n",
    "\t\"\"\" A basic Perceptron (the same pcn.py except with the weights printed\n",
    "\tand it does not reorder the inputs)\"\"\"\n",
    "\t\n",
    "\tdef __init__(self,inputs,targets):\n",
    "\t\t\"\"\" Constructor \"\"\"\n",
    "\t\t# Set up network size\n",
    "\t\tif np.ndim(inputs)>1:\n",
    "\t\t\tself.nIn = np.shape(inputs)[1]\n",
    "\t\telse: \n",
    "\t\t\tself.nIn = 1\n",
    "\t\n",
    "\t\tif np.ndim(targets)>1:\n",
    "\t\t\tself.nOut = np.shape(targets)[1]\n",
    "\t\telse:\n",
    "\t\t\tself.nOut = 1\n",
    "\n",
    "\t\tself.nData = np.shape(inputs)[0]\n",
    "\t\n",
    "\t\t# Initialise network\n",
    "\t\tself.weights = np.random.rand(self.nIn+1,self.nOut)*0.1-0.05\n",
    "\n",
    "\tdef pcntrain(self,inputs,targets,eta,nIterations):\n",
    "\t\t\"\"\" Train the thing \"\"\"\t\n",
    "\t\t# Add the inputs that match the bias node\n",
    "\t\tinputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "\t\n",
    "\t\t# Training\n",
    "\t\tchange = range(self.nData)\n",
    "\n",
    "\t\tfor n in range(nIterations):\n",
    "\t\t\t\n",
    "\t\t\tself.activations = self.pcnfwd(inputs);\n",
    "\t\t\tself.weights -= eta*np.dot(np.transpose(inputs),self.activations-targets)\n",
    "\t\t\tprint(\"Iteration: \", n)\n",
    "\t\t\tprint(self.weights)\n",
    "\t\t\t\n",
    "\t\t\tactivations = self.pcnfwd(inputs)\n",
    "\t\t\tprint(\"Final outputs are:\")\n",
    "\t\t\tprint(activations)\n",
    "\t\t#return self.weights\n",
    "\n",
    "\tdef pcnfwd(self,inputs):\n",
    "\t\t\"\"\" Run the network forward \"\"\"\n",
    "\n",
    "\t\t# Compute activations\n",
    "\t\tactivations =  np.dot(inputs,self.weights)\n",
    "\n",
    "\t\t# Threshold the activations\n",
    "\t\treturn np.where(activations>0,1,0)\n",
    "\n",
    "\tdef confmat(self,inputs,targets):\n",
    "\t\t\"\"\"Confusion matrix\"\"\"\n",
    "\n",
    "\t\t# Add the inputs that match the bias node\n",
    "\t\tinputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "\t\toutputs = np.dot(inputs,self.weights)\n",
    "\t\n",
    "\t\tnClasses = np.shape(targets)[1]\n",
    "\n",
    "\t\tif nClasses==1:\n",
    "\t\t\tnClasses = 2\n",
    "\t\t\toutputs = np.where(outputs>0,1,0)\n",
    "\t\telse:\n",
    "\t\t\t# 1-of-N encoding\n",
    "\t\t\toutputs = np.argmax(outputs,1)\n",
    "\t\t\ttargets = np.argmax(targets,1)\n",
    "\n",
    "\t\tcm = np.zeros((nClasses,nClasses))\n",
    "\t\tfor i in range(nClasses):\n",
    "\t\t\tfor j in range(nClasses):\n",
    "\t\t\t\tcm[i,j] = np.sum(np.where(outputs==i,1,0)*np.where(targets==j,1,0))\n",
    "\n",
    "\t\tprint(cm)\n",
    "\t\tprint(np.trace(cm)/np.sum(cm))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhBrxMb3fvpZ"
   },
   "source": [
    "Below you see 4 data points with a target value (0 or 1) for each data point. We need to predict the target value. We plot this with a red color for target 0 and blue for target 1.\n",
    "\n",
    "Predicting the target here means finding a straight line such that all red points are on one side of the line and the blue points on the other side of the line. Since this example is very simple, you can draw the line yourself (I hope...). This simplicity helps us to understand what the algorithm does.\n",
    "\n",
    "We define a tensor with 4 `inputs` and 4 `targets`. The first element of `inputs` (which has two coordinates) corresponds to the first element in `targets`.\n",
    "\n",
    "To refer to the points in the figure, we use \"standard\" terminology $x$ and $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1138,
     "status": "ok",
     "timestamp": 1559233378416,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "ZvRsrHvKfNtj",
    "outputId": "57c888ac-dc01-4bcf-d24f-fc812ecf7be4"
   },
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "targets = np.array([[0],[1],[1],[1]])\n",
    "colors = []\n",
    "for i in range(len(targets)):\n",
    "    colors.append(['red','blue'][targets[i][0]])\n",
    "plt.scatter(inputs[:,0],inputs[:,1],color=colors)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7p2jAEXcMHT"
   },
   "source": [
    "Now we use the `pcn` class and instantiate it with the `inputs` and `targets` above. We call `p` and instance of the class `pcn`. In this course we will not dwell on the object oriented aspects of python, so do not worry if you do not fully understand this. However, as you will see below, when we use tensorflow, we have a similar structure: we create an instance of a tensorflow class.\n",
    "\n",
    "The function definition is `pcntrain(self,inputs,targets,eta,nIterations)`. Do not worry about the `self` part. We call the function as `pcntrain(inputs,targets,0.25,6)`. Hence, the first two arguments of the function are our `inputs` and `targets` tensors. \n",
    "\n",
    "**Continue**\n",
    "\n",
    "The result is that `p` is now associated with our tensors `inputs` and `targets`. Now we can use the function (\"method\") `pcntrain` to train the network. After each iteration, the algorithm prints some information. Looking at `pcntrain` above, we can see that the information that is printed is:\n",
    "\n",
    "```\n",
    "print(\"Iteration: \", n)\n",
    "print(self.weights)\n",
    "print(\"Final outputs are:\")\n",
    "print(activations)\n",
    "```\n",
    "\n",
    "So first we see the number of the iteration (and python starts counting at 0). Then we see three numbers that correspond to the weights. The weights determine the line in the figure above. In particular, if we write the weights tensor as $w = (w_0,w_1,w_2)$, then a line in the figure is given by $w_0 x + w_1 y =w_2$. Equivalently, we can write this as:\n",
    "\n",
    "\\begin{equation}\n",
    "y = (w_2-w_0 x)/w_1\n",
    "\\end{equation}\n",
    "\n",
    "Next, we see the final outputs (targets): this is the \"prediction\" of the algorithm for the vector `targets`. Recall that `targets` is of the form [0,1,1,1]. Hence, after the first iteration, we incorrectly label the first point as 1 while it should be 0. Hence, the algorithm continues and generates different (and ultimately better) predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1559237984092,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "63uEOtIWfN6V",
    "outputId": "45a109d8-acc7-4eb6-8cae-e35c6af42ee0"
   },
   "outputs": [],
   "source": [
    "p = pcn(inputs,targets)\n",
    "p.pcntrain(inputs,targets,0.25,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKOWDBWi0-CU"
   },
   "source": [
    "The final weights $w$ are now equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1559238001077,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "fYYxwxkFfOEU",
    "outputId": "7c823942-0cdc-4008-884b-27362a258f7d"
   },
   "outputs": [],
   "source": [
    "p.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1Bwm4bMgDf0"
   },
   "source": [
    "We can now plot the line $y=(w_2-w_0 x)/w_1$. Indeed, as you can see, the line sepates the red point (below the line) from the blue points (above the line). If we give the algorithm a point above the line, it will predict \"blue\", if we give it a point below the line it will predict \"red\". \n",
    "\n",
    "Because we only have 4 points here, this is arbitrary for points close to the line: we can shift the line, still separate the points, but get different predictions for points close to the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1559238186300,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "McHF9dYYgTzN",
    "outputId": "f915397f-d00c-4f6f-848e-fedc4c609e93"
   },
   "outputs": [],
   "source": [
    "x0 = 0\n",
    "x1 = 1\n",
    "y0 = (p.weights[2]-p.weights[0]*x0)/p.weights[1]\n",
    "y1 = (p.weights[2]-p.weights[0]*x1)/p.weights[1]\n",
    "plt.scatter(inputs[:,0],inputs[:,1],color=colors)\n",
    "plt.plot([x0,x1],[y0,y1],'k') #we only need to give two points to plot a line\n",
    "plt.xlabel('$x$')\n",
    "plt.xlabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PhQ1VJw5OyE"
   },
   "source": [
    "We can see from the graph and check with the function `pcnfwd` below what the predicted labels will be for the points (0.5,0.5) and (0.5,-0.5). The `-1` is needed for the \"constant\" $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1559239211184,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "-ce5DGnTQOuN",
    "outputId": "79942d7f-3bb5-4df4-ac55-900a18473ac5"
   },
   "outputs": [],
   "source": [
    "p.pcnfwd([[0.5,0.5,-1],[0.5,-0.5,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ud5MlICH5yoE"
   },
   "source": [
    "## Multi-layer perceptron\n",
    "\n",
    "With more than one layer, we get into deep learning.\n",
    "\n",
    "We use [this mlp code](http://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch4/mlp.py) from Chapter 4 of Machine Learning: An Algorithmic Perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUgtItNk5Tx9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Code from Chapter 4 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class mlp:\n",
    "    \"\"\" A Multi-Layer Perceptron\"\"\"\n",
    "    \n",
    "    def __init__(self,inputs,targets,nhidden,beta=1,momentum=0.9,outtype='logistic'):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        # Set up network size\n",
    "        self.nin = np.shape(inputs)[1]\n",
    "        self.nout = np.shape(targets)[1]\n",
    "        self.ndata = np.shape(inputs)[0]\n",
    "        self.nhidden = nhidden\n",
    "\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.outtype = outtype\n",
    "    \n",
    "        # Initialise network\n",
    "        self.weights1 = (np.random.rand(self.nin+1,self.nhidden)-0.5)*2/np.sqrt(self.nin)\n",
    "        self.weights2 = (np.random.rand(self.nhidden+1,self.nout)-0.5)*2/np.sqrt(self.nhidden)\n",
    "\n",
    "    def earlystopping(self,inputs,targets,valid,validtargets,eta,niterations=100):\n",
    "    \n",
    "        valid = np.concatenate((valid,-np.ones((np.shape(valid)[0],1))),axis=1)\n",
    "        \n",
    "        old_val_error1 = 100002\n",
    "        old_val_error2 = 100001\n",
    "        new_val_error = 100000\n",
    "        \n",
    "        count = 0\n",
    "        while (((old_val_error1 - new_val_error) > 0.001) or ((old_val_error2 - old_val_error1)>0.001)):\n",
    "            count+=1\n",
    "            print(count)\n",
    "            self.mlptrain(inputs,targets,eta,niterations)\n",
    "            old_val_error2 = old_val_error1\n",
    "            old_val_error1 = new_val_error\n",
    "            validout = self.mlpfwd(valid)\n",
    "            new_val_error = 0.5*np.sum((validtargets-validout)**2)\n",
    "            \n",
    "        print(\"Stopped\", new_val_error,old_val_error1, old_val_error2)\n",
    "        return new_val_error\n",
    "    \t\n",
    "    def mlptrain(self,inputs,targets,eta,niterations):\n",
    "        \"\"\" Train the thing \"\"\"    \n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((self.ndata,1))),axis=1)\n",
    "        change = range(self.ndata)\n",
    "    \n",
    "        updatew1 = np.zeros((np.shape(self.weights1)))\n",
    "        updatew2 = np.zeros((np.shape(self.weights2)))\n",
    "            \n",
    "        for n in range(niterations):\n",
    "    \n",
    "            self.outputs = self.mlpfwd(inputs)\n",
    "\n",
    "            error = 0.5*np.sum((self.outputs-targets)**2)\n",
    "            if (np.mod(n,100)==0):\n",
    "                print(\"Iteration: \",n, \" Error: \",error)\n",
    "\n",
    "            # Different types of output neurons\n",
    "            if self.outtype == 'linear':\n",
    "            \tdeltao = (self.outputs-targets)/self.ndata\n",
    "            elif self.outtype == 'logistic':\n",
    "            \tdeltao = self.beta*(self.outputs-targets)*self.outputs*(1.0-self.outputs)\n",
    "            elif self.outtype == 'softmax':\n",
    "                deltao = (self.outputs-targets)*(self.outputs*(-self.outputs)+self.outputs)/self.ndata \n",
    "            else:\n",
    "            \tprint(\"error\")\n",
    "            \n",
    "            deltah = self.hidden*self.beta*(1.0-self.hidden)*(np.dot(deltao,np.transpose(self.weights2)))\n",
    "                      \n",
    "            updatew1 = eta*(np.dot(np.transpose(inputs),deltah[:,:-1])) + self.momentum*updatew1\n",
    "            updatew2 = eta*(np.dot(np.transpose(self.hidden),deltao)) + self.momentum*updatew2\n",
    "            self.weights1 -= updatew1\n",
    "            self.weights2 -= updatew2\n",
    "                \n",
    "            # Randomise order of inputs (not necessary for matrix-based calculation)\n",
    "            #np.random.shuffle(change)\n",
    "            #inputs = inputs[change,:]\n",
    "            #targets = targets[change,:]\n",
    "            \n",
    "    def mlpfwd(self,inputs):\n",
    "        \"\"\" Run the network forward \"\"\"\n",
    "\n",
    "        self.hidden = np.dot(inputs,self.weights1);\n",
    "        self.hidden = 1.0/(1.0+np.exp(-self.beta*self.hidden))\n",
    "        self.hidden = np.concatenate((self.hidden,-np.ones((np.shape(inputs)[0],1))),axis=1)\n",
    "\n",
    "        outputs = np.dot(self.hidden,self.weights2);\n",
    "\n",
    "        # Different types of output neurons\n",
    "        if self.outtype == 'linear':\n",
    "        \treturn outputs\n",
    "        elif self.outtype == 'logistic':\n",
    "            return 1.0/(1.0+np.exp(-self.beta*outputs))\n",
    "        elif self.outtype == 'softmax':\n",
    "            normalisers = np.sum(np.exp(outputs),axis=1)*np.ones((1,np.shape(outputs)[0]))\n",
    "            return np.transpose(np.transpose(np.exp(outputs))/normalisers)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "    def confmat(self,inputs,targets):\n",
    "        \"\"\"Confusion matrix\"\"\"\n",
    "\n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((np.shape(inputs)[0],1))),axis=1)\n",
    "        outputs = self.mlpfwd(inputs)\n",
    "        \n",
    "        nclasses = np.shape(targets)[1]\n",
    "\n",
    "        if nclasses==1:\n",
    "            nclasses = 2\n",
    "            outputs = np.where(outputs>0.5,1,0)\n",
    "        else:\n",
    "            # 1-of-N encoding\n",
    "            outputs = np.argmax(outputs,1)\n",
    "            targets = np.argmax(targets,1)\n",
    "\n",
    "        cm = np.zeros((nclasses,nclasses))\n",
    "        for i in range(nclasses):\n",
    "            for j in range(nclasses):\n",
    "                cm[i,j] = np.sum(np.where(outputs==i,1,0)*np.where(targets==j,1,0))\n",
    "\n",
    "        print(\"Confusion matrix is:\")\n",
    "        print(cm)\n",
    "        print(\"Percentage Correct: \",np.trace(cm)/np.sum(cm)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-R0tbZB6nPk"
   },
   "source": [
    "This code is more elaborate than the perceptron above. It also introduces new concepts like the confusion matrix. We will go over these below. \n",
    "\n",
    "Before we use tensorflow with pandas dataframes, we first use more primitive datastructures in `numpy`. This serves two purposes: first, it is simple and helps to understand the underlying python code and algorithms; second, sometimes the data that you get is too messy to fit into a dataframe right away. Then it is useful to know some more primitive structures as well.\n",
    "\n",
    "Below we read the famous [iris dataset](http://archive.ics.uci.edu/ml/datasets/Iris). Admittedly, flowers are not directly linked to economics, but this is one of the classic datasets in classification. You cannot claim to have done \"datascience\" without having looked at the iris dataset.\n",
    "\n",
    "We will first download it from the website, just to learn the relevant steps here. We will work with in numpy and then we turn it into a pandas dataframe. In fact, the data is so famous that it is included in the tensorflow library, but later you may want to analyze data that is not included in any library. Hence, we need to learn the download steps as well.\n",
    "\n",
    "When you look at the downloaded data (in an editor), you will notice that it actually includes the names as strings. For us it is easier to work with numerical labels 0,1,2. Hence, we use the function `preprocessIris`. This function takes two arguments: the input file with the data that we downloaded and the output file where the target-labels are replaced by 0,1,2.\n",
    "\n",
    "If you run this notebook for the first time, un-comment the line `preprocessIris('iris.data','iris_proc.data')` (that is, remove the hashtag \"#\" at the beginning of the line), where `iris.data` is the name of the file that you downloaded and `iris_proc.data` is the name of the file with labels 0,1,2. Once the file `iris_proc.data` is created, there is no need to run this python command again.\n",
    "\n",
    "**continue here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1558971173290,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "1hiC6IOj5TqT",
    "outputId": "f00b572d-391f-420f-e0f7-ebcb57dbcd64"
   },
   "outputs": [],
   "source": [
    "# Code from Chapter 4 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "# The iris classification example\n",
    "\n",
    "def preprocessIris(infile,outfile):\n",
    "\n",
    "    stext1 = 'Iris-setosa'\n",
    "    stext2 = 'Iris-versicolor'\n",
    "    stext3 = 'Iris-virginica'\n",
    "    rtext1 = '0'\n",
    "    rtext2 = '1'\n",
    "    rtext3 = '2'\n",
    "\n",
    "    fid = open(infile,\"r\")\n",
    "    oid = open(outfile,\"w\")\n",
    "\n",
    "    for s in fid:\n",
    "        if s.find(stext1)>-1:\n",
    "            oid.write(s.replace(stext1, rtext1))\n",
    "        elif s.find(stext2)>-1:\n",
    "            oid.write(s.replace(stext2, rtext2))\n",
    "        elif s.find(stext3)>-1:\n",
    "            oid.write(s.replace(stext3, rtext3))\n",
    "    fid.close()\n",
    "    oid.close()\n",
    "\n",
    "## Preprocessor to remove the test (only needed once)\n",
    "# preprocessIris('iris.data','iris_proc.data')\n",
    "\n",
    "iris = np.loadtxt('./data/iris_proc.data',delimiter=',')\n",
    "iris[:,:4] = iris[:,:4]-iris[:,:4].mean(axis=0)\n",
    "imax = np.concatenate((iris.max(axis=0)*np.ones((1,5)),np.abs(iris.min(axis=0)*np.ones((1,5)))),axis=0).max(axis=0)\n",
    "iris[:,:4] = iris[:,:4]/imax[:4]\n",
    "print(iris[0:5,:])\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "target = np.zeros((np.shape(iris)[0],3));\n",
    "indices = np.where(iris[:,4]==0) \n",
    "target[indices,0] = 1\n",
    "indices = np.where(iris[:,4]==1)\n",
    "target[indices,1] = 1\n",
    "indices = np.where(iris[:,4]==2)\n",
    "target[indices,2] = 1\n",
    "\n",
    "# Randomly order the data\n",
    "order = np.arange(np.shape(iris)[0])\n",
    "np.random.shuffle(order)\n",
    "iris = iris[order,:]\n",
    "target = target[order,:]\n",
    "\n",
    "train = iris[::2,0:4]\n",
    "traint = target[::2]\n",
    "valid = iris[1::4,0:4]\n",
    "validt = target[1::4]\n",
    "test = iris[3::4,0:4]\n",
    "testt = target[3::4]\n",
    "\n",
    "#print train.max(axis=0), train.min(axis=0)\n",
    "\n",
    "# Train the network\n",
    "#import mlp\n",
    "net = mlp(train,traint,5,outtype='logistic')\n",
    "net.earlystopping(train,traint,valid,validt,0.1)\n",
    "net.confmat(test,testt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3iUynAm8cpC"
   },
   "outputs": [],
   "source": [
    "preprocessIris('./data/iris.data','./data/iris_proc2.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5352
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1558977591543,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "bTRd17Ds5Teh",
    "outputId": "deb8cc99-ca09-4afb-dd19-1c8b640373d1"
   },
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H1yCdGFW4j_F"
   },
   "source": [
    "# Premade Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PS6_yKSoyLAl"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/estimators/premade_estimators\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/estimators/premade_estimators.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/tree/master/site/en/r2/tutorials/estimators/premade_estimators.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4YZ_ievcY7p"
   },
   "source": [
    "\n",
    "This tutorial shows you\n",
    "how to solve the Iris classification problem in TensorFlow using Estimators. An Estimator is TensorFlow's high-level representation of a complete model, and it has been designed for easy scaling and asynchronous training. For more details see\n",
    "[Estimators](https://www.tensorflow.org/guide/estimators).\n",
    "\n",
    "Note that in TensorFlow 2.0, the [Keras API](https://www.tensorflow.org/guide/keras) can accomplish many of these same tasks, and is believed to be an easier API to learn. If you are starting fresh, we would recommend you start with Keras. For more information about the available high level APIs in TensorFlow 2.0, see [Standardizing on Keras](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IFct0yedsTy"
   },
   "source": [
    "## First things first\n",
    "\n",
    "In order to get started, you will first import TensorFlow and a number of libraries you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qO3Vs97ehiMZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5w4m5gncnGh"
   },
   "source": [
    "## The data set\n",
    "\n",
    "The sample program in this document builds and tests a model that\n",
    "classifies Iris flowers into three different species based on the size of their\n",
    "[sepals](https://en.wikipedia.org/wiki/Sepal) and\n",
    "[petals](https://en.wikipedia.org/wiki/Petal).\n",
    "\n",
    "\n",
    "You will train a model using the Iris data set. The Iris data set contains four features and one\n",
    "[label](https://developers.google.com/machine-learning/glossary/#label).\n",
    "The four features identify the following botanical characteristics of\n",
    "individual Iris flowers:\n",
    "\n",
    "* sepal length\n",
    "* sepal width\n",
    "* petal length\n",
    "* petal width\n",
    "\n",
    "Based on this information, you can define a few helpful constants for parsing the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSyrXp_He_UE"
   },
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6mTfIQzfC9w"
   },
   "source": [
    "Next, download and parse the Iris data set using Keras and Pandas. Note that you keep distinct datasets for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS-GobzcZIxi"
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'SepalLength':iris[::2,0],'SepalWidth':iris[::2,1],'PetalLength':iris[::2,2],'PetalWidth':iris[::2,3],'Species':iris[::2,4]})\n",
    "valid = pd.DataFrame({'SepalLength':iris[1::4,0],'SepalWidth':iris[1::4,1],'PetalLength':iris[1::4,2],'PetalWidth':iris[1::4,3],'Species':iris[1::4,4]})\n",
    "test = pd.DataFrame({'SepalLength':iris[3::4,0],'SepalWidth':iris[3::4,1],'PetalLength':iris[3::4,2],'PetalWidth':iris[3::4,3],'Species':iris[3::4,4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCWZGd2jbKRA"
   },
   "outputs": [],
   "source": [
    "train['Species']=train.Species.astype(int)\n",
    "valid['Species']=valid.Species.astype(int)\n",
    "test['Species']=test.Species.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1558979697574,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "wRCrAZg8aAsR",
    "outputId": "e89de982-062a-4bd5-bc44-2c9dd68b6f26"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1558979701097,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "hNWuDd-mSPrH",
    "outputId": "e5843528-51bf-42d8-c41f-cfcb6c2e18a5"
   },
   "outputs": [],
   "source": [
    "train_y = train.pop('Species')\n",
    "valid_y = valid.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "\n",
    "# The label column has now been removed from the features.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHFxNLszhQjz"
   },
   "source": [
    "You can inspect your data to see that you have four float feature columns and one int32 label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZx1L_1Vcmxv"
   },
   "source": [
    "## Overview of programming with Estimators\n",
    "\n",
    "Now that you have the data set up, you can define a model using a TensorFlow Estimator. An Estimator is any class derived from `tf.estimator.Estimator`. TensorFlow\n",
    "provides a collection of\n",
    "`tf.estimator`\n",
    "(for example, `LinearRegressor`) to implement common ML algorithms. Beyond\n",
    "those, you may write your own\n",
    "[custom Estimators](https://www.tensorflow.org/guide/custom_estimators).\n",
    "We recommend using pre-made Estimators when just getting started.\n",
    "\n",
    "To write a TensorFlow program based on pre-made Estimators, you must perform the\n",
    "following tasks:\n",
    "\n",
    "* Create one or more input functions.\n",
    "* Define the model's feature columns.\n",
    "* Instantiate an Estimator, specifying the feature columns and various\n",
    "  hyperparameters.\n",
    "* Call one or more methods on the Estimator object, passing the appropriate\n",
    "  input function as the source of the data.\n",
    "\n",
    "Let's see how those tasks are implemented for Iris classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2OcguDfBcmmg"
   },
   "source": [
    "## Create input functions\n",
    "\n",
    "You must create input functions to supply data for training,\n",
    "evaluating, and prediction.\n",
    "\n",
    "An **input function** is a function that returns a `tf.data.Dataset` object\n",
    "which outputs the following two-element tuple:\n",
    "\n",
    "* [`features`](https://developers.google.com/machine-learning/glossary/#feature) - A Python dictionary in which:\n",
    "    * Each key is the name of a feature.\n",
    "    * Each value is an array containing all of that feature's values.\n",
    "* `label` - An array containing the values of the\n",
    "  [label](https://developers.google.com/machine-learning/glossary/#label) for\n",
    "  every example.\n",
    "\n",
    "Just to demonstrate the format of the input function, here's a simple\n",
    "implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzr5vRr5caGF"
   },
   "outputs": [],
   "source": [
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1558979703930,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "T8BIzD3lhcLh",
    "outputId": "ce5d497d-3322-4a77-88a8-b3aaf1ec9abe"
   },
   "outputs": [],
   "source": [
    "input_evaluation_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpXvGjfnjHgY"
   },
   "source": [
    "Your input function may generate the `features` dictionary and `label` list any\n",
    "way you like. However, we recommend using TensorFlow's [Dataset API](https://www.tensorflow.org/guide/datasets), which can\n",
    "parse all sorts of data.\n",
    "\n",
    "The Dataset API can handle a lot of common cases for you. For example,\n",
    "using the Dataset API, you can easily read in records from a large collection\n",
    "of files in parallel and join them into a single stream.\n",
    "\n",
    "To keep things simple in this example you are going to load the data with\n",
    "[pandas](https://pandas.pydata.org/), and build an input pipeline from this\n",
    "in-memory data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T20u1anCi8NP"
   },
   "outputs": [],
   "source": [
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIwcFT4MlZEi"
   },
   "source": [
    "## Define the feature columns\n",
    "\n",
    "A [**feature column**](https://developers.google.com/machine-learning/glossary/#feature_columns)\n",
    "is an object describing how the model should use raw input data from the\n",
    "features dictionary. When you build an Estimator model, you pass it a list of\n",
    "feature columns that describes each of the features you want the model to use.\n",
    "The `tf.feature_column` module provides many options for representing data\n",
    "to the model.\n",
    "\n",
    "For Iris, the 4 raw features are numeric values, so we'll build a list of\n",
    "feature columns to tell the Estimator model to represent each of the four\n",
    "features as 32-bit floating-point values. Therefore, the code to create the\n",
    "feature column is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTTriO8FlSML"
   },
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpKkhMoZljco"
   },
   "source": [
    "Feature columns can be far more sophisticated than those we're showing here.  You can read more about Feature Columns in [this guide](https://www.tensorflow.org/guide/feature_columns).\n",
    "\n",
    "Now that you have the description of how you want the model to represent the raw\n",
    "features, you can build the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuE59XHEl22K"
   },
   "source": [
    "## Instantiate an estimator\n",
    "\n",
    "The Iris problem is a classic classification problem. Fortunately, TensorFlow\n",
    "provides several pre-made classifier Estimators, including:\n",
    "\n",
    "* `tf.estimator.DNNClassifier` for deep models that perform multi-class\n",
    "  classification.\n",
    "* `tf.estimator.DNNLinearCombinedClassifier` for wide & deep models.\n",
    "* `tf.estimator.LinearClassifier` for classifiers based on linear models.\n",
    "\n",
    "For the Iris problem, `tf.estimator.DNNClassifier` seems like the best choice.\n",
    "Here's how you instantiated this Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1558979708400,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "qnf4o2V5lcPn",
    "outputId": "d5b184ac-172a-4cb3-b8b7-55028463c26d"
   },
   "outputs": [],
   "source": [
    "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[30, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzzt5nUpmEe3"
   },
   "source": [
    "## Train, Evaluate, and Predict\n",
    "\n",
    "Now that you have an Estimator object, you can call methods to do the following:\n",
    "\n",
    "* Train the model.\n",
    "* Evaluate the trained model.\n",
    "* Use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnihuLdWmE75"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Train the model by calling the Estimator's `train` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12884,
     "status": "ok",
     "timestamp": 1558979723603,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "4jW08YtPl1iS",
    "outputId": "32ecc7a3-7e21-4ae9-da52-51b9435b15be"
   },
   "outputs": [],
   "source": [
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
    "    steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ybiTFDmlmes8"
   },
   "source": [
    "Note that you wrap up your `input_fn` call in a\n",
    "[`lambda`](https://docs.python.org/3/tutorial/controlflow.html)\n",
    "to capture the arguments while providing an input function that takes no\n",
    "arguments, as expected by the Estimator. The `steps` argument tells the method\n",
    "to stop training after a number of training steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNvJLH8hmsdf"
   },
   "source": [
    "### Evaluate the trained model\n",
    "\n",
    "Now that the model has been trained, you can get some statistics on its\n",
    "performance. The following code block evaluates the accuracy of the trained\n",
    "model on the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1393,
     "status": "ok",
     "timestamp": 1558979731315,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "A169XuO4mKxF",
    "outputId": "8788688d-ef65-4e03-c70d-4fe22e46db8d"
   },
   "outputs": [],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz3UgZflbv4F"
   },
   "source": [
    "on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1558979830900,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "GbMls6Cxbx0E",
    "outputId": "7f65be6d-95ce-4fdb-d728-dfca5d78cc29"
   },
   "outputs": [],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(valid, valid_y, training=False))\n",
    "\n",
    "print('\\nValidation set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnPMP5EHph17"
   },
   "source": [
    "Unlike the call to the `train` method, you did not pass the `steps`\n",
    "argument to evaluate. The `input_fn` for eval only yields a single\n",
    "[epoch](https://developers.google.com/machine-learning/glossary/#epoch) of data.\n",
    "\n",
    "\n",
    "The `eval_result` dictionary also contains the `average_loss` (mean loss per sample), the `loss` (mean loss per mini-batch) and the value of the estimator's `global_step` (the number of training iterations it underwent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ur624ibpp52X"
   },
   "source": [
    "### Making predictions (inferring) from the trained model\n",
    "\n",
    "You now have a trained model that produces good evaluation results.\n",
    "You can now use the trained model to predict the species of an Iris flower\n",
    "based on some unlabeled measurements. As with training and evaluation, you make\n",
    "predictions using a single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wltc0jpgng38"
   },
   "outputs": [],
   "source": [
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "def input_fn(features, batch_size=256):\n",
    "    \"\"\"An input function for prediction.\"\"\"\n",
    "    # Convert the inputs to a Dataset without labels.\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda: input_fn(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JsETKQo0rHvi"
   },
   "source": [
    "The `predict` method returns a Python iterable, yielding a dictionary of\n",
    "prediction results for each example. The following code prints a few\n",
    "predictions and their probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 437,
     "status": "error",
     "timestamp": 1558979910651,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Efm4mLzkrCxp",
    "outputId": "c2dfe0bd-cfbd-4ef8-a4cf-126956f6a9ed"
   },
   "outputs": [],
   "source": [
    "for pred_dict, expec in zip(valid, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\n",
    "        SPECIES[class_id], 100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nWvDdeBhyxt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msqzofYyQPIl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KyDLbOdPORji"
   },
   "source": [
    "# Treatment effects\n",
    "\n",
    "Machine learning is about predicting outcomes. When the prediction is accurate (out-of-sample) we are happy, almost no matter what the model looks like.\n",
    "\n",
    "This is different when we are interested in policy advice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EugUUVOCfM3n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsZ3XVXNOo3R"
   },
   "source": [
    "## IV\n",
    "\n",
    "What are instrumental variables again?\n",
    "\n",
    "We use the IV example in [Richard McElreath's lecture 18\n",
    "](https://www.youtube.com/watch?v=e5cgiAGBKzI) (starting around 28:00) which is based on Angrist and Krueger (1991).\n",
    "\n",
    "An individual's education (`e`) affects her wage (`w`). We want to know how strong this effect is.\n",
    "\n",
    "`q` denotes fraction of the year that has elapsed when you were born.\n",
    "\n",
    "**TODO** add text\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcYm0YSJvF-U"
   },
   "outputs": [],
   "source": [
    "N_observations=200\n",
    "alpha_w = 1.\n",
    "beta_ew = 0.\n",
    "alpha_e = 1.\n",
    "beta_qe = 2.\n",
    "q = tf.random.uniform([N_observations])\n",
    "u = tf.random.normal([N_observations])\n",
    "e = alpha_e + beta_qe*q + u + tf.random.normal([N_observations])\n",
    "w = alpha_w + beta_ew*e + u + tf.random.normal([N_observations])\n",
    "df = pd.DataFrame({'q':q,'e':e,'w':w})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1556370363732,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "BQHnN4GKvr3Q",
    "outputId": "964e2164-d547-4e3f-bcaf-129b8b5f2ea1"
   },
   "outputs": [],
   "source": [
    "results = smf.ols('w ~ e', data=df).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1556370364979,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "PDC_1QTB2772",
    "outputId": "5e48fc3f-d239-41f4-b3e1-577a08483d58"
   },
   "outputs": [],
   "source": [
    "\n",
    "mod = IV2SLS.from_formula('w ~ 1 + [e ~ q]', df)\n",
    "\n",
    "iv_res = mod.fit()\n",
    "iv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1556370366607,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "2pa4d0D25lEG",
    "outputId": "5cde68e6-7b80-4132-e1c8-103646f2974d"
   },
   "outputs": [],
   "source": [
    "iv_res.first_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterogenous treatment effects\n",
    "\n",
    "One of the complications when using IV to find causal effects is that treatment effects can differ between subgroups and that these subgroups react differently to the treatment/instrument. This is based on Chapter 4 of Angrist and Pischke (2009).\n",
    "\n",
    "We will consider this in the context of unemployed who are offered a training program. To understand the effects, we will again generate our own data such that we know exactly what all the relevant effects are. The question is then, how do we get these effects from the data.\n",
    "\n",
    "As before, the learning goals of this section are two fold: (i) learn how to program your own data generating process in python and (ii) understand the statistical issue under considerarion: heterogenous treatment effects.\n",
    "\n",
    "The government introduces a training program for unemployed. The treatment variable $D_i$ denotes whether (1) or not (0) individual $i$ received training. The instrument $Z_i$ that we use is whether (1) or not (0) $i$ is invited to join the training. \n",
    "\n",
    "We have three groups of unemployed:\n",
    "\n",
    "* compliers join the training if and only if they are invited to do so: $D_i = 1 (0)$ iff $Z_i = 1 (0)$;\n",
    "* always-takers join the training whether or not they are invited to do so: $D_i =1$ irrespective of $Z_i$;\n",
    "* never-takers never join the training: $D_i = 0$ irrespective of $Z_i$.\n",
    "\n",
    "Note that in this training context, the always-takers are a bit \"odd\": how can you join a training program where you are not supposed to be. The never-takers make more sense in this context: some people may not attend the training even though they were invited to do so. We will not worry about this; the point is that we can conceptually distinguish these groups.\n",
    "\n",
    "We are interested in the effect of the training program on earnings. We denote by $\\beta$ the expected earnings without training. The expected earnings with training is denoted by $\\beta+\\tau$. Hence, $\\tau$ denotes the treatment effects.\n",
    "\n",
    "Let $j \\in \\{c,a,n \\}$ denote whether an individual is a complier, always-taker or never-taker. We assume that $\\beta_j$ and $\\tau_j$ vary with $j$.\n",
    "\n",
    "In particular we assume that the earnings effect for someone in group $j$ is drawn from a normal distribution with mean $\\mu = \\beta_j$ and standard deviation $\\sigma =1$ in case of no training and the effect of training is drawn from a normal distribution with mean $\\mu = \\tau_j$ and standard deviation $\\sigma =1$.\n",
    "\n",
    "Below we will consider different configurations for $\\beta_j, \\tau_j$ and see how they affect the estimated effects.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_complier(invited):\n",
    "    return invited\n",
    "\n",
    "def accept_always(invited):\n",
    "    return np.ones_like(invited)\n",
    "\n",
    "def accept_never(invited):\n",
    "    return np.zeros_like(invited)\n",
    "\n",
    "N_simulations = 10000\n",
    "fraction_training = 0.2 # 20% of agents are invited to the training\n",
    "types = ['complier','always_taker', 'never_taker']\n",
    "β = {'complier': 1, 'always_taker': 2, 'never_taker': 1}\n",
    "τ = {'complier': 3, 'always_taker': 4, 'never_taker': 1}\n",
    "n = {'complier': 0.5, 'always_taker': 0.25, 'never_taker': 0.25}\n",
    "accept = {'complier': accept_complier, 'always_taker': accept_always, 'never_taker': accept_never}\n",
    "earnings_without = {}\n",
    "training_effect = {}\n",
    "earnings = {}\n",
    "invited = {}\n",
    "trained = {}\n",
    "df = pd.DataFrame()\n",
    "def data_simulation(β = β, τ=τ, n = n):\n",
    "    for j in types:\n",
    "        earnings_without[j] = np.random.normal(loc = β[j], scale = 1.0, size = int(n[j]*N_simulations))\n",
    "        training_effect[j] = np.random.normal(loc = τ[j], scale = 1.0, size = int(n[j]*N_simulations))\n",
    "        invited[j] = np.random.binomial(1,fraction_training,size = int(n[j]*N_simulations))\n",
    "        trained[j] = accept[j](invited[j])\n",
    "        earnings[j] = earnings_without[j]+ trained[j]*training_effect[j]\n",
    "    column_invited = np.concatenate([invited[j] for j in types],axis=0)\n",
    "    column_trained = np.concatenate([trained[j] for j in types],axis=0)\n",
    "    column_earnings = np.concatenate([earnings[j] for j in types],axis=0)\n",
    "    df = pd.DataFrame({'invited':column_invited, 'trained':column_trained,'earnings':column_earnings})\n",
    "    return df\n",
    " \n",
    "df = data_simulation()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset now with indicators for who was invited to a training and who attended a training.\n",
    "\n",
    "We would like to learn the effect of training on earnings.\n",
    "\n",
    "**Question** Which of the parameters above could we hope to recover? $\\tau_c,\\tau_a,\\tau_n$?\n",
    "\n",
    "**Question** Hence, if we do some calculations with the data, which number do we want to see?\n",
    "\n",
    "**Question** Compare the average earnings of people with training with the average earnings of people without training. Is this the number that you expected? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "972c29cbe1eb1b5f97ffd536ae75c0e6",
     "grade": true,
     "grade_id": "cell-11dfbcb44332c071",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2565d0d65888895a0eac304a316e80b0",
     "grade": true,
     "grade_id": "cell-acf4003b13766093",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Where does this number come from?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2ed895dd0e230560fccf01f9dfd95137",
     "grade": true,
     "grade_id": "cell-dd52aff093968cda",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Perhaps we should use the instrument whether someone if invited or not: compare average earnings of people with and without an invitation to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "24aab6fb90e967c127e46ab591418e4b",
     "grade": true,
     "grade_id": "cell-9db7815c2398153f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Does it help if we compare:\n",
    "* people with training and an invitation to training with\n",
    "* people without training and no invitation to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c251646103d4a036053fb24dfb874d3e",
     "grade": true,
     "grade_id": "cell-49c080f49b9fd497",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f773bf19bba897aa231cfb8c90c5020",
     "grade": true,
     "grade_id": "cell-c821f614ca4069ce",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have seen now that we have no hope of recovering the relevant training effect in the set-up above. So the question is: when can we recover the relevant parameters? That is, what assumptions are needed?\n",
    "\n",
    "To see how you can analyze this with the set-up above, let's program two obvious cases where this works.\n",
    "\n",
    "**Question** Explain for each case *why* it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bca6bf6a7e9d35b8da14ccc7b24d61e8",
     "grade": false,
     "grade_id": "cell-3442a89a6492e55a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = {'complier': 1.0, 'always_taker': 0.0, 'never_taker': 0.0}\n",
    "df_compliers_only = data_simulation(n=n)\n",
    "np.mean(df_compliers_only[df_compliers_only.trained==1].earnings)-np.mean(df_compliers_only[df_compliers_only.trained==0].earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7f720b238236580cb600387924d5290",
     "grade": false,
     "grade_id": "cell-88d8277560e25fce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "β = {'complier': 1, 'always_taker': 1, 'never_taker': 1}\n",
    "τ = {'complier': 3, 'always_taker': 3, 'never_taker': 3}\n",
    "df_homog = data_simulation(β=β,τ=τ)\n",
    "np.mean(df_homog[df_homog.trained==1].earnings)-np.mean(df_homog[df_homog.trained==0].earnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "342655ea47f720e7780bbba1cc0b907b",
     "grade": true,
     "grade_id": "cell-8f6f6af99170937d",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a less obvious case where we can retrieve relevant parameters, we will use Angrist and Pischke (2009: chapter 4). In particular, we will verify Theorem 4.4.2.\n",
    "\n",
    "We work with one sided noncompliance. That is, we allow for never-takers but not for always-takers. \n",
    "\n",
    "According to Theorem 4.4.2 the effect of treatment on the treated can be found as:\n",
    "\n",
    "$$\n",
    "\\frac{E(Y_i|Z_i=1)-E(Y_i|Z_i=0)}{Prob(D_i=1|Z_i=1)}\n",
    "$$\n",
    "\n",
    "We first create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = {'complier': 0.5, 'always_taker': 0.0, 'never_taker': 0.5}\n",
    "df_one_sided = data_simulation(n=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Calculate $Prob(D_i = 1|Z_i =1)$ and denote this `correction_term`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "296dd73324f49824f8f5a70680587af8",
     "grade": true,
     "grade_id": "cell-e9b3b577d5be8379",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Calculate $\\frac{E(Y_i|Z_i=1)-E(Y_i|Z_i=0)}{Prob(D_i=1|Z_i=1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f4c932bc8f7ca66e806e47c25f9c7d1e",
     "grade": true,
     "grade_id": "cell-ce2c5964e6ea8f99",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Which parameter have we recovered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b03cf0308b269e45b0d747802e85698d",
     "grade": true,
     "grade_id": "cell-1cf09bb679b872df",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of treatment\n",
    "\n",
    "Consider a situation which is simpler than the one above in the sense that there are no always-takers, not never-takers.\n",
    "\n",
    "However, now the instrument (\"invited\") $Z$ causes the probability of training to increase from 0.2 to 0.6. 30% of people get invited.\n",
    "\n",
    "**Exercise** Copy/paste python code from the analysis above to generate a dataframe using that $Z_i=1$ implies that the probability of treatment increases from 0.2 to 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d4bf900ee2943ac1227971df69add8a",
     "grade": true,
     "grade_id": "cell-c97483a41eb8d21f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Suppose we can observe whether someone received training or not. Use this to calculate the effect of training on earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "578369e281de4c420d4e68db2935c994",
     "grade": true,
     "grade_id": "cell-6625d613915b1528",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we cannot observe whether someone actually did the training or not (e.g. whether or not someone invested effort in the training). We only know (\"from a previous study\") that explicitly inviting someone for the training (\"nudge\") increases the probability that they invest training effort from 0.2 to 0.6.\n",
    "\n",
    "**Question** Use the dataframe above to verify what the effect of the nudge is. Call this the `correction_term`. What value should the correction term have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d08af4ee287458ca332c35f5bc18a231",
     "grade": true,
     "grade_id": "cell-556b7cd2f434cb69",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angrist and Pischke (2009: chapter 4) Theorem 4.4.1 (LATE Theorem; local average treatment effect) claims that (under some assumptions), we can calculate the effect of training on earnings as:\n",
    "\n",
    "$$\n",
    "\\frac{E(Y_i|Z_i=1)-E(Y_i|Z_i=0)}{E(D_i|Z_i=1)-E(D_i|Z_i=0)}\n",
    "$$\n",
    "\n",
    "**Question** Use this expression to calculate the effect of training on earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c94a601523d07429524894524ac0fba3",
     "grade": true,
     "grade_id": "cell-907f393d8be6a0b5",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use an IV estimation to find the same result. Run the following two regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_first_stage = smf.ols('trained ~ invited', data=df_prob).fit()\n",
    "results_second_stage = smf.ols('earnings ~ invited', data=df_prob).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Use these regressions to calculate the effect of training on earning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d9e02af0f1f02c4f7386fc8f01f6bc43",
     "grade": true,
     "grade_id": "cell-85b062063df389f9",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Statistical_Hacking.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
